# -*- coding: utf-8 -*-
"""M8_DM1_164221112_164221114_164221117_164221118_164221120

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12mA-Hj4JU0UXPP_YzDoSQFG8bxeRIefT


### **IMPORT LIBRARY**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE

"""### **IMPORT DATASET**"""

df = pd.read_csv("/content/bread basket.csv")
df.head()

"""### **CHECK UNIQUE VALUE**"""

print(df["Item"].unique())

print(df["period_day"].unique())

print(df["weekday_weekend"].unique())

print(df["Transaction"].unique())

"""
### **CHECK MISSING VALUE**"""

missing_values = df.isna().sum()
print(missing_values)

"""*  Tidak ada missing value pada data"""

!pip install mlxtend
!pip install pyfpgrowth

"""#### **FEATURE SELECTION**"""

# Menghapus kolom 'weekday_weekend', dan 'period_day'
df = df.drop(columns=['weekday_weekend', 'period_day'])

# Menyaring dan menghapus 'bags' dari deskripsi item
df = df[df['Item'] != 'bags']

# Mengubah 'date_time' menjadi format datetime
df['date_time'] = pd.to_datetime(df['date_time'], format='%d-%m-%Y %H:%M')

# Mengelompokkan semua item yang terjual pada tanggal yang sama ke dalam satu daftar
df = df.groupby(df['date_time'].dt.date)['Item'].apply(list).reset_index()

# Mengubah nama kolom untuk kejelasan
df.columns = ['Tanggal', 'Item']

# Menampilkan beberapa baris pertama dari dataset yang sudah diproses
print(df.head())

# Membuat daftar kosong untuk menyimpan transaksi
transaksi = []

# Melakukan iterasi pada setiap baris di DataFrame
for indeks, baris in df.iterrows():
    # Menambahkan daftar item pada tanggal tertentu ke dalam daftar 'transaksi'
    transaksi.append(baris['Item'])

# Sekarang 'transaksi' berisi daftar item yang dibeli pada setiap tanggal, mewakili transaksi individu

"""#### **TRAINING THE MODEL**

#### **Apriori Algorithm**
"""

!pip install apyori

from apyori import apriori
# Menggunakan apriori untuk mencari aturan asosiasi
rules = apriori(transactions=transaksi, min_support=0.00412087912, min_confidence=0.6, min_lift=1.9, min_length=2, max_length=2)

from apyori import apriori
import pandas as pd

# Menampilkan hasil langsung dari output fungsi apriori
results = list(apriori(transactions=transaksi, min_support=0.00412087912, min_confidence=0.6, min_lift=1.9, min_length=2, max_length=2))

# Mengatur hasil ke dalam DataFrame Pandas
def inspect(results):
    lhs         = [tuple(result[2][0][0])[0] for result in results]  # Item di sisi kiri
    rhs         = [tuple(result[2][0][1])[0] for result in results]  # Item di sisi kanan
    supports    = [result[1] for result in results]  # Nilai dukungan
    confidences = [result[2][0][2] for result in results]  # Nilai kepercayaan
    lifts       = [result[2][0][3] for result in results]  # Nilai lift
    return list(zip(lhs, rhs, supports, confidences, lifts))
resultsDataFrame = pd.DataFrame(inspect(results), columns=['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])

# Menampilkan hasil yang diurutkan dari nilai kepercayaan tertinggi ke terendah
resultsDataFrame.sort_values('Confidence', ascending=False)

"""* Dengan menggunakan Apriori Algorithm, aturan asosiasi antara 'Adjustment' dan 'Bread Pudding' menjadi aturan asosiasi terbaik untuk asosiasi antar kelompok, dibuktikan dengan nilai confidence dan lift tertinggi dibandingkan dengan asosiasi lainnya.                                                          


* Nilai confidence sebesar 100% menunjukkan bahwa jika ada pembelian 'Adjustment', kemungkinan besar akan ada pembelian 'Bread Pudding' dalam transaksi yang sama. Nilai lift sebesar 79.500000 menggambarkan asosiasi yang cukup kuat antara dua kelompok item tersebut.
"""

import pandas as pd

# Memilih hanya dua kolom pertama
resultFrame = resultsDataFrame.iloc[:, 0:-3]

# Menampilkan frame hasil sementara
resultFrame

# Mengelompokkan semua item yang terjual bersama pada tanggal yang sama ke dalam satu kolom
resultFrame = resultFrame.groupby('Left Hand Side')['Right Hand Side'].apply(list).reset_index()

# Mengganti nama kolom untuk lebih jelas
resultFrame = resultFrame.rename(columns={'Left Hand Side': 'antecedent', 'Right Hand Side': 'consequent'})

# Menambahkan kolom confidence dan support
resultFrame['Support'] = resultsDataFrame['Support']
resultFrame['Confidence'] = resultsDataFrame['Confidence']

# Menampilkan frame hasil akhir
resultFrame

"""#### **FP-Growth**"""

!pip install mlxtend

"""##### Transformation Data"""

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import fpgrowth

# Pastikan path ke file CSV benar dan file memiliki kolom 'Transaction' dan 'Item'
df = pd.read_csv("/content/bread basket.csv")

# Cek apakah kolom 'Transaction' ada dalam DataFrame
if 'Transaction' in df.columns and 'Item' in df.columns:
    # Mengelompokkan data berdasarkan 'Transaction' dan mengumpulkan item-item dalam satu transaksi
    grouped = df.groupby('Transaction')['Item'].apply(list).tolist()

    # Persiapan data transaksi untuk algoritma FP-Growth
    encoder = TransactionEncoder()
    encoded_data = encoder.fit_transform(grouped)
    df_encoded = pd.DataFrame(encoded_data, columns=encoder.columns_)

"""##### Displays frequently occurring items"""

# Jalankan FP-Growth untuk menemukan itemsets yang sering muncul
frequent_itemsets = fpgrowth(df_encoded, min_support=0.01, use_colnames=True)

# Tampilkan itemsets yang sering muncul
print(frequent_itemsets.head())

"""* Dengan menggunakan FP-Growth, 'Bread' adalah item yang sering dibeli konsumen. Hal ini dibuktikan dengan nilai support tertinggi sebesar 0.327205, yang berarti sekitar 32.7% dari transaksi mencakup pembelian ini.

##### Generate association rules
"""

from mlxtend.frequent_patterns import association_rules

# Menghasilkan aturan asosiasi
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.1)

# Tampilkan aturan yang dihasilkan
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())

"""* Dengan menggunakan FP-Growth Algorithm, aturan asosiasi antara 'Bread' dan 'Coffee' menjadi aturan asosiasi terbaik untuk asosiasi antar kelompok, ditunjukkan oleh nilai confidence dan lift tertinggi dibandingkan dengan asosiasi lainnya.

* Nilai confidence sebesar 27.51% menunjukkan bahwa jika ada pembelian 'Bread', kemungkinan besar akan ada pembelian 'Coffee' dalam transaksi yang sama. Nilai lift sebesar 0.575059 menunjukkan bahwa pembelian 'Bread' sedikit lebih cenderung terjadi bersamaan dengan pembelian 'Coffee', tetapi asosiasi ini tidak terlalu kuat.

##### Uses association rules for recommendations
"""

def recommend_products(basket, rules):
    recommendations = []
    basket_set = set(basket)
    for index, row in rules.iterrows():
        if row['antecedents'].issubset(basket_set):
            recommendations.append(row['consequents'])
    return recommendations

# Contoh keranjang belanja
example_basket = ['Bread', 'Jam']
recommended_products = recommend_products(example_basket, rules)
print(f"Produk yang direkomendasikan untuk {example_basket}: {recommended_products}")

"""* Produk yang direkomendasikan berdasarkan keranjang belanja ['Bread', 'Jam'] adalah **'Coffee'**. Hal ini sesuai dengan aturan asosiasi yang telah ditemukan, di mana pembelian 'Bread' cenderung terjadi bersamaan dengan pembelian 'Coffee'.

#### **ECLAT**
"""

# Memuat data
df = pd.read_csv("/content/bread basket.csv")

# Mengelompokkan item berdasarkan 'Transaction'
transaction_items = df.groupby('Transaction')['Item'].apply(set).tolist()

import pandas as pd
from itertools import combinations, chain

# Fungsi untuk menghitung support
def calculate_support(itemset, transaction_data):
    return sum(1 for transaction in transaction_data if itemset.issubset(transaction)) / len(transaction_data)

# Fungsi untuk mengekstrak itemsets yang sering menggunakan ECLAT
def eclat(transaction_data, min_support):
    # Inisialisasi itemset dengan item tunggal dan support mereka
    itemsets = {frozenset([item]): calculate_support(frozenset([item]), transaction_data)
                for transaction in transaction_data for item in transaction}
    itemsets = {item: support for item, support in itemsets.items() if support >= min_support}

    # Menyimpan itemsets yang sering
    frequent_itemsets = dict(itemsets)

    # Kombinasi itemset
    def combine_sets(current_itemsets, k):
        return set(frozenset(chain(*pair)) for pair in combinations(current_itemsets, 2) if len(frozenset(chain(*pair))) == k)

    # Mencari kombinasi yang sering
    k = 2
    while itemsets:
        # Menggabungkan itemsets dengan ukuran k dan menghitung support
        current_combinations = combine_sets(itemsets.keys(), k)
        itemsets = {items: calculate_support(items, transaction_data) for items in current_combinations}
        # Menyaring kombinasi dengan support minimal
        itemsets = {item: support for item, support in itemsets.items() if support >= min_support}
        # Menambahkan itemset yang sering ke hasil
        frequent_itemsets.update(itemsets)
        k += 1

    return frequent_itemsets
# Menentukan nilai minimum support
min_support = 0.01

# Menjalankan ECLAT
frequent_itemsets = eclat(transaction_items, min_support)

# Menampilkan hasil semua item tanpa menyebutkan satu-satu
for itemset, support in frequent_itemsets.items():
    print(f"Itemset: {list(itemset)}, Support: {support}")

"""Dengan menggunakan ECLAT, 'Coffee' adalah item yang sering dibeli konsumen. Hal ini dibuktikan dengan nilai support tertinggi sebesar 0.478394, yang berarti sekitar 47.8% dari transaksi mencakup pembelian ini.



"""

import pandas as pd
from itertools import combinations, chain

# Fungsi untuk menghitung support
def calculate_support(itemset, transaction_data):
    return sum(1 for transaction in transaction_data if itemset.issubset(transaction)) / len(transaction_data)

# Fungsi untuk mengekstrak itemsets yang sering menggunakan ECLAT
def eclat(transaction_data, min_support):
    # Inisialisasi itemset dengan item tunggal dan support mereka
    itemsets = {frozenset([item]): calculate_support(frozenset([item]), transaction_data)
                for transaction in transaction_data for item in transaction}
    itemsets = {item: support for item, support in itemsets.items() if support >= min_support}

    # Menyimpan itemsets yang sering
    frequent_itemsets = dict(itemsets)

    # Kombinasi itemset
    def combine_sets(current_itemsets, k):
        return set(frozenset(chain(*pair)) for pair in combinations(current_itemsets, 2) if len(frozenset(chain(*pair))) == k)

    # Mencari kombinasi yang sering
    k = 2
    while itemsets:
        # Menggabungkan itemsets dengan ukuran k dan menghitung support
        current_combinations = combine_sets(itemsets.keys(), k)
        itemsets = {items: calculate_support(items, transaction_data) for items in current_combinations}
        # Menyaring kombinasi dengan support minimal
        itemsets = {item: support for item, support in itemsets.items() if support >= min_support}
        # Menambahkan itemset yang sering ke hasil
        frequent_itemsets.update(itemsets)
        k += 1

    return frequent_itemsets

# Fungsi untuk menghitung confidence
def calculate_confidence(frequent_itemsets, antecedent, consequent):
    antecedent_support = frequent_itemsets.get(antecedent, 0)
    consequent_support = frequent_itemsets.get(consequent, 0)
    support_both = frequent_itemsets.get(antecedent.union(consequent), 0)
    if antecedent_support == 0 or support_both == 0:
        return 0
    return support_both / antecedent_support

# Menentukan nilai minimum support
min_support = 0.01

# Menjalankan ECLAT
frequent_itemsets = eclat(transaction_items, min_support)

# Membuat DataFrame untuk hasil aturan asosiasi
rules = []
for itemset, support in frequent_itemsets.items():
    if len(itemset) > 1:
        for antecedent, consequent in combinations(itemset, 2):
            confidence = calculate_confidence(frequent_itemsets, frozenset([antecedent]), frozenset([consequent]))
            rules.append((antecedent, consequent, support, confidence))

rules_df = pd.DataFrame(rules, columns=["Antecedent", "Consequent", "Support", "Confidence"])

# Menampilkan hasil
print(rules_df)

"""* Dengan menggunakan ECLAT Algorithm, aturan asosiasi antara 'Toast' dan 'Coffee' menjadi aturan asosiasi terbaik untuk asosiasi antar kelompok, ditunjukkan oleh nilai confidence tertinggi dibandingkan dengan asosiasi lainnya.

* Nilai confidence sebesar 70.44% menunjukkan bahwa jika ada pembelian 'Toast', kemungkinan besar akan ada pembelian 'Coffee' dalam transaksi yang sama
"""